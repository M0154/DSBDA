{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c2e85dd8-bcfa-40d3-be54-b91f833b0faa",
   "metadata": {},
   "source": [
    "Assignment-5\n",
    "Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "a. Data preparation\n",
    "b. Generate training data\n",
    "c. Train model\n",
    "d. Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5540cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e596b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking random sentences as data\n",
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "dl_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae494900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 75\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d236309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating (context word, target/label word) pairs\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "            \n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        # print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449d4937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#model building\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(cbow.summary())\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99dac9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000020893F24720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x0000020893F24720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch: 1 \tLoss: 429.0836968421936\n",
      "\n",
      "Epoch: 2 \tLoss: 429.647958278656\n",
      "\n",
      "Epoch: 3 \tLoss: 427.86786222457886\n",
      "\n",
      "Epoch: 4 \tLoss: 426.17456912994385\n",
      "\n",
      "Epoch: 5 \tLoss: 424.7746205329895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accf5b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0.009806</td>\n",
       "      <td>-0.016295</td>\n",
       "      <td>-0.011585</td>\n",
       "      <td>0.007788</td>\n",
       "      <td>0.045331</td>\n",
       "      <td>-0.027111</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>-0.029176</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.020728</td>\n",
       "      <td>0.058328</td>\n",
       "      <td>-0.013742</td>\n",
       "      <td>-0.042865</td>\n",
       "      <td>-0.022896</td>\n",
       "      <td>-0.050835</td>\n",
       "      <td>0.037517</td>\n",
       "      <td>0.027861</td>\n",
       "      <td>0.012320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>0.019918</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>0.056956</td>\n",
       "      <td>0.051058</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>-0.045634</td>\n",
       "      <td>-0.025223</td>\n",
       "      <td>-0.017124</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033404</td>\n",
       "      <td>0.060911</td>\n",
       "      <td>0.015303</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>-0.008355</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>-0.048719</td>\n",
       "      <td>0.065349</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>-0.025176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>-0.004682</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>-0.040814</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>-0.028351</td>\n",
       "      <td>0.018611</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>-0.015008</td>\n",
       "      <td>-0.027600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013518</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.033790</td>\n",
       "      <td>-0.046902</td>\n",
       "      <td>-0.007777</td>\n",
       "      <td>-0.045835</td>\n",
       "      <td>-0.014123</td>\n",
       "      <td>-0.013749</td>\n",
       "      <td>-0.038048</td>\n",
       "      <td>0.036759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.016735</td>\n",
       "      <td>-0.009090</td>\n",
       "      <td>0.032796</td>\n",
       "      <td>0.037050</td>\n",
       "      <td>-0.025276</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>-0.044795</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>-0.046154</td>\n",
       "      <td>-0.024880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048237</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.043496</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>-0.007433</td>\n",
       "      <td>-0.010193</td>\n",
       "      <td>-0.007328</td>\n",
       "      <td>0.011880</td>\n",
       "      <td>-0.039523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.010303</td>\n",
       "      <td>0.026749</td>\n",
       "      <td>0.036683</td>\n",
       "      <td>-0.040839</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>-0.019370</td>\n",
       "      <td>-0.016945</td>\n",
       "      <td>-0.031613</td>\n",
       "      <td>-0.035602</td>\n",
       "      <td>0.031528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.032113</td>\n",
       "      <td>-0.044521</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>-0.005708</td>\n",
       "      <td>-0.031745</td>\n",
       "      <td>-0.049437</td>\n",
       "      <td>-0.047617</td>\n",
       "      <td>0.040496</td>\n",
       "      <td>0.008396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "deep      0.009806 -0.016295 -0.011585  0.007788  0.045331 -0.027111   \n",
       "networks  0.019918 -0.039554  0.056956  0.051058 -0.032229  0.009455   \n",
       "neural   -0.004682  0.029678 -0.040814  0.005468 -0.028351  0.018611   \n",
       "and       0.016735 -0.009090  0.032796  0.037050 -0.025276  0.031447   \n",
       "as        0.010303  0.026749  0.036683 -0.040839  0.004997 -0.019370   \n",
       "\n",
       "                6         7         8         9   ...        90        91  \\\n",
       "deep      0.012627  0.033691 -0.029176  0.000855  ...  0.027748  0.020728   \n",
       "networks -0.045634 -0.025223 -0.017124  0.036002  ...  0.033404  0.060911   \n",
       "neural    0.001819  0.024518 -0.015008 -0.027600  ... -0.013518  0.019277   \n",
       "and      -0.044795  0.009937 -0.046154 -0.024880  ... -0.048237  0.033169   \n",
       "as       -0.016945 -0.031613 -0.035602  0.031528  ...  0.000763  0.032113   \n",
       "\n",
       "                92        93        94        95        96        97  \\\n",
       "deep      0.058328 -0.013742 -0.042865 -0.022896 -0.050835  0.037517   \n",
       "networks  0.015303  0.025187 -0.008355 -0.010786 -0.048719  0.065349   \n",
       "neural    0.033790 -0.046902 -0.007777 -0.045835 -0.014123 -0.013749   \n",
       "and       0.040698  0.043496 -0.007086 -0.007433 -0.010193 -0.007328   \n",
       "as       -0.044521  0.034603 -0.005708 -0.031745 -0.049437 -0.047617   \n",
       "\n",
       "                98        99  \n",
       "deep      0.027861  0.012320  \n",
       "networks -0.014286 -0.025176  \n",
       "neural   -0.038048  0.036759  \n",
       "and       0.011880 -0.039523  \n",
       "as        0.040496  0.008396  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d8aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707908a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55cff7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1190 - loss: 2.5607\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 2.5545\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 2.5483\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.1667 - loss: 2.5422\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1905 - loss: 2.5360\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.2143 - loss: 2.5299\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2381 - loss: 2.5238\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2381 - loss: 2.5176\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2857 - loss: 2.5115\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2857 - loss: 2.5054\n",
      "Words similar to 'learning': ['machine', 'is', 'various', 'fascinating', 'fields']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Sample text data\n",
    "text = \"Machine learning is fascinating and continuously evolving with applications in various fields.\"\n",
    "\n",
    "# Stage 1: Data Preparation\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "vocab_size = len(word2index) + 1  # +1 for padding token\n",
    "\n",
    "# Convert text to sequences of word indices\n",
    "sequences = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Define context window size\n",
    "window_size = 2  # Number of context words on each side of the target word\n",
    "\n",
    "# Stage 2: Generate Training Data\n",
    "# Create pairs of context and target words\n",
    "def generate_context_target_pairs(sequences, window_size):\n",
    "    context_target_pairs = []\n",
    "    for i, target in enumerate(sequences):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(sequences), i + window_size + 1)\n",
    "        context = [sequences[j] for j in range(start, end) if j != i]\n",
    "        for ctx_word in context:\n",
    "            context_target_pairs.append((ctx_word, target))\n",
    "    return context_target_pairs\n",
    "\n",
    "context_target_pairs = generate_context_target_pairs(sequences, window_size)\n",
    "\n",
    "# Convert pairs to input-output format for model training\n",
    "X_train, y_train = zip(*context_target_pairs)\n",
    "X_train = np.array(X_train)\n",
    "y_train = to_categorical(y_train, vocab_size)\n",
    "\n",
    "# Stage 3: Define and Train the CBOW Model\n",
    "embedding_dim = 100  # Dimension of the word embeddings\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=256)\n",
    "\n",
    "# Stage 4: Output: Check Word Embeddings\n",
    "# Get the embeddings\n",
    "embedding_layer = model.layers[0]\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Display similar words based on cosine similarity\n",
    "def get_similar_words(word, embeddings, top_n=5):\n",
    "    index = word2index[word]\n",
    "    word_embedding = embeddings[index]\n",
    "    similarities = [(other_word, np.dot(word_embedding, embeddings[other_index]))\n",
    "                    for other_word, other_index in word2index.items() if other_word != word]\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Test the model by finding similar words\n",
    "test_word = \"learning\"\n",
    "similar_words = get_similar_words(test_word, embeddings)\n",
    "print(f\"Words similar to '{test_word}':\", [word for word, _ in similar_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27224ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
